{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar-10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sartkap/Pytorch-ML-algos/blob/master/cifar_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NMDsrzp_6vgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9aKLGhNTQUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4yt_52tvGxOe",
        "colab_type": "code",
        "outputId": "2b36cce9-427b-4658-e088-cef804b5eca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download= True,transform=transform)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XNmgsTsLJZsF",
        "colab_type": "code",
        "outputId": "d36de935-2805-4aef-cd7b-23549b9b77ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HF8Quw8Y_yIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_set,batch_size = 128,shuffle=True,num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set,batch_size=4,shuffle=False,num_workers=2)#what does last parameter mean?\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0UYvypO-I7o3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    #syntax is (Num_input_channels,Num_output_channels,filter_size,stride,padding)\n",
        "    self.conv1 = nn.Conv2d(3,16,3,1,1)\n",
        "    self.conv2 = nn.Conv2d(16,16,3,1,1)\n",
        "    self.conv3 = nn.Conv2d(16,16,3,1,1)\n",
        "    self.conv4 = nn.Conv2d(16,16,3,1,1)\n",
        "    self.conv5 = nn.Conv2d(16,16,3,1,1)\n",
        "    self.conv6 = nn.Conv2d(16,16,3,1,1)\n",
        "    self.conv7 = nn.Conv2d(16,16,3,1,1)\n",
        "    #assert()\n",
        "    self.conv8 = nn.Conv2d(16,32,3,2,1)\n",
        "    self.conv9 = nn.Conv2d(32,32,3,1,1)\n",
        "    self.conv10 = nn.Conv2d(32,32,3,1,1)\n",
        "    self.conv11 = nn.Conv2d(32,32,3,1,1)\n",
        "    self.conv12 = nn.Conv2d(32,32,3,1,1)\n",
        "    self.conv13 = nn.Conv2d(32,32,3,1,1)\n",
        "    self.conv14 = nn.Conv2d(32,64,3,2,1)\n",
        "    self.conv15 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.conv16 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.conv17 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.conv18 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.conv19 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.conv20 = nn.Conv2d(64,64,3,1,1)\n",
        "    self.pool1 = nn.AvgPool2d(8)\n",
        "    self.fully_connected1 = nn.Linear(64,10)\n",
        "    #syntax is (pool_filter_size,stride,padding) for max_pool(not used now)\n",
        "    \n",
        "      \n",
        "  def forward(self,x):\n",
        "    x = func.relu(self.conv1(x))\n",
        "    temp = x\n",
        "    x = func.relu(self.conv2(x))\n",
        "    x = func.relu(self.conv3(x)+temp)\n",
        "    temp = x\n",
        "    x = func.relu(self.conv4(x))\n",
        "    x = func.relu(self.conv5(x)+temp)\n",
        "    temp = x\n",
        "    x = func.relu(self.conv6(x))\n",
        "    x = func.relu(self.conv7(x)+temp)\n",
        "    x = func.relu(self.conv8(x))\n",
        "    temp=x\n",
        "    x = func.relu(self.conv9(x))\n",
        "    x = func.relu(self.conv10(x)+temp)\n",
        "    temp = x\n",
        "    x = func.relu(self.conv11(x))\n",
        "    x = func.relu(self.conv12(x)+temp)\n",
        "    #temp = x\n",
        "    x = func.relu(self.conv13(x))\n",
        "    x = func.relu(self.conv14(x))\n",
        "    temp = x\n",
        "    x = func.relu(self.conv15(x))\n",
        "    x = func.relu(self.conv16(x)+temp)\n",
        "    temp = x\n",
        "    x = func.relu(self.conv17(x))\n",
        "    x = func.relu(self.conv18(x)+temp)\n",
        "    temp = x\n",
        "    x = func.relu(self.conv19(x))\n",
        "    x = func.relu(self.conv20(x)+temp)\n",
        "    x = self.pool1(x)\n",
        "    x = x.reshape(x.size(0),-1)\n",
        "    x = torch.sigmoid(self.fully_connected1(x))\n",
        "    return x\n",
        "model = Network()          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKuk514mJS5g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(),0.03,0.9,0.0001)#(,learning rates, momentum, weight decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e2StigCnK8hg",
        "colab_type": "code",
        "outputId": "79eaf2bd-3af3-4336-fd5d-9ea6f10568f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3143
        }
      },
      "cell_type": "code",
      "source": [
        "n_batches = len(trainloader)\n",
        "for epoch in range(20):\n",
        "  print_var = n_batches // 10\n",
        "  running_loss = 0.0\n",
        "  for i,train_img in enumerate(trainloader):\n",
        "    inputs, labels = train_img\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if (i+1)%(1+print_var)==0:\n",
        "      print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} \".format(epoch+1, int(100 * (i+1) / n_batches), running_loss /print_var))\n",
        "      running_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, 10% \t train_loss: 1.84 \n",
            "Epoch 1, 20% \t train_loss: 1.85 \n",
            "Epoch 1, 30% \t train_loss: 1.84 \n",
            "Epoch 1, 40% \t train_loss: 1.82 \n",
            "Epoch 1, 51% \t train_loss: 1.83 \n",
            "Epoch 1, 61% \t train_loss: 1.83 \n",
            "Epoch 1, 71% \t train_loss: 1.83 \n",
            "Epoch 1, 81% \t train_loss: 1.82 \n",
            "Epoch 1, 92% \t train_loss: 1.81 \n",
            "Epoch 2, 10% \t train_loss: 1.80 \n",
            "Epoch 2, 20% \t train_loss: 1.82 \n",
            "Epoch 2, 30% \t train_loss: 1.81 \n",
            "Epoch 2, 40% \t train_loss: 1.80 \n",
            "Epoch 2, 51% \t train_loss: 1.80 \n",
            "Epoch 2, 61% \t train_loss: 1.79 \n",
            "Epoch 2, 71% \t train_loss: 1.80 \n",
            "Epoch 2, 81% \t train_loss: 1.80 \n",
            "Epoch 2, 92% \t train_loss: 1.81 \n",
            "Epoch 3, 10% \t train_loss: 1.80 \n",
            "Epoch 3, 20% \t train_loss: 1.80 \n",
            "Epoch 3, 30% \t train_loss: 1.79 \n",
            "Epoch 3, 40% \t train_loss: 1.79 \n",
            "Epoch 3, 51% \t train_loss: 1.80 \n",
            "Epoch 3, 61% \t train_loss: 1.79 \n",
            "Epoch 3, 71% \t train_loss: 1.80 \n",
            "Epoch 3, 81% \t train_loss: 1.79 \n",
            "Epoch 3, 92% \t train_loss: 1.80 \n",
            "Epoch 4, 10% \t train_loss: 1.78 \n",
            "Epoch 4, 20% \t train_loss: 1.80 \n",
            "Epoch 4, 30% \t train_loss: 1.79 \n",
            "Epoch 4, 40% \t train_loss: 1.78 \n",
            "Epoch 4, 51% \t train_loss: 1.80 \n",
            "Epoch 4, 61% \t train_loss: 1.78 \n",
            "Epoch 4, 71% \t train_loss: 1.78 \n",
            "Epoch 4, 81% \t train_loss: 1.79 \n",
            "Epoch 4, 92% \t train_loss: 1.78 \n",
            "Epoch 5, 10% \t train_loss: 1.78 \n",
            "Epoch 5, 20% \t train_loss: 1.78 \n",
            "Epoch 5, 30% \t train_loss: 1.78 \n",
            "Epoch 5, 40% \t train_loss: 1.79 \n",
            "Epoch 5, 51% \t train_loss: 1.77 \n",
            "Epoch 5, 61% \t train_loss: 1.78 \n",
            "Epoch 5, 71% \t train_loss: 1.77 \n",
            "Epoch 5, 81% \t train_loss: 1.77 \n",
            "Epoch 5, 92% \t train_loss: 1.78 \n",
            "Epoch 6, 10% \t train_loss: 1.77 \n",
            "Epoch 6, 20% \t train_loss: 1.76 \n",
            "Epoch 6, 30% \t train_loss: 1.76 \n",
            "Epoch 6, 40% \t train_loss: 1.77 \n",
            "Epoch 6, 51% \t train_loss: 1.77 \n",
            "Epoch 6, 61% \t train_loss: 1.76 \n",
            "Epoch 6, 71% \t train_loss: 1.76 \n",
            "Epoch 6, 81% \t train_loss: 1.76 \n",
            "Epoch 6, 92% \t train_loss: 1.77 \n",
            "Epoch 7, 10% \t train_loss: 1.75 \n",
            "Epoch 7, 20% \t train_loss: 1.76 \n",
            "Epoch 7, 30% \t train_loss: 1.75 \n",
            "Epoch 7, 40% \t train_loss: 1.76 \n",
            "Epoch 7, 51% \t train_loss: 1.75 \n",
            "Epoch 7, 61% \t train_loss: 1.76 \n",
            "Epoch 7, 71% \t train_loss: 1.75 \n",
            "Epoch 7, 81% \t train_loss: 1.76 \n",
            "Epoch 7, 92% \t train_loss: 1.76 \n",
            "Epoch 8, 10% \t train_loss: 1.76 \n",
            "Epoch 8, 20% \t train_loss: 1.76 \n",
            "Epoch 8, 30% \t train_loss: 1.75 \n",
            "Epoch 8, 40% \t train_loss: 1.74 \n",
            "Epoch 8, 51% \t train_loss: 1.75 \n",
            "Epoch 8, 61% \t train_loss: 1.74 \n",
            "Epoch 8, 71% \t train_loss: 1.75 \n",
            "Epoch 8, 81% \t train_loss: 1.75 \n",
            "Epoch 8, 92% \t train_loss: 1.75 \n",
            "Epoch 9, 10% \t train_loss: 1.74 \n",
            "Epoch 9, 20% \t train_loss: 1.75 \n",
            "Epoch 9, 30% \t train_loss: 1.75 \n",
            "Epoch 9, 40% \t train_loss: 1.74 \n",
            "Epoch 9, 51% \t train_loss: 1.75 \n",
            "Epoch 9, 61% \t train_loss: 1.75 \n",
            "Epoch 9, 71% \t train_loss: 1.75 \n",
            "Epoch 9, 81% \t train_loss: 1.74 \n",
            "Epoch 9, 92% \t train_loss: 1.74 \n",
            "Epoch 10, 10% \t train_loss: 1.74 \n",
            "Epoch 10, 20% \t train_loss: 1.74 \n",
            "Epoch 10, 30% \t train_loss: 1.74 \n",
            "Epoch 10, 40% \t train_loss: 1.76 \n",
            "Epoch 10, 51% \t train_loss: 1.75 \n",
            "Epoch 10, 61% \t train_loss: 1.73 \n",
            "Epoch 10, 71% \t train_loss: 1.74 \n",
            "Epoch 10, 81% \t train_loss: 1.75 \n",
            "Epoch 10, 92% \t train_loss: 1.74 \n",
            "Epoch 11, 10% \t train_loss: 1.73 \n",
            "Epoch 11, 20% \t train_loss: 1.73 \n",
            "Epoch 11, 30% \t train_loss: 1.73 \n",
            "Epoch 11, 40% \t train_loss: 1.73 \n",
            "Epoch 11, 51% \t train_loss: 1.74 \n",
            "Epoch 11, 61% \t train_loss: 1.73 \n",
            "Epoch 11, 71% \t train_loss: 1.75 \n",
            "Epoch 11, 81% \t train_loss: 1.75 \n",
            "Epoch 11, 92% \t train_loss: 1.74 \n",
            "Epoch 12, 10% \t train_loss: 1.73 \n",
            "Epoch 12, 20% \t train_loss: 1.73 \n",
            "Epoch 12, 30% \t train_loss: 1.73 \n",
            "Epoch 12, 40% \t train_loss: 1.73 \n",
            "Epoch 12, 51% \t train_loss: 1.73 \n",
            "Epoch 12, 61% \t train_loss: 1.73 \n",
            "Epoch 12, 71% \t train_loss: 1.73 \n",
            "Epoch 12, 81% \t train_loss: 1.72 \n",
            "Epoch 12, 92% \t train_loss: 1.74 \n",
            "Epoch 13, 10% \t train_loss: 1.72 \n",
            "Epoch 13, 20% \t train_loss: 1.73 \n",
            "Epoch 13, 30% \t train_loss: 1.73 \n",
            "Epoch 13, 40% \t train_loss: 1.72 \n",
            "Epoch 13, 51% \t train_loss: 1.72 \n",
            "Epoch 13, 61% \t train_loss: 1.71 \n",
            "Epoch 13, 71% \t train_loss: 1.72 \n",
            "Epoch 13, 81% \t train_loss: 1.72 \n",
            "Epoch 13, 92% \t train_loss: 1.72 \n",
            "Epoch 14, 10% \t train_loss: 1.72 \n",
            "Epoch 14, 20% \t train_loss: 1.71 \n",
            "Epoch 14, 30% \t train_loss: 1.71 \n",
            "Epoch 14, 40% \t train_loss: 1.72 \n",
            "Epoch 14, 51% \t train_loss: 1.71 \n",
            "Epoch 14, 61% \t train_loss: 1.72 \n",
            "Epoch 14, 71% \t train_loss: 1.73 \n",
            "Epoch 14, 81% \t train_loss: 1.72 \n",
            "Epoch 14, 92% \t train_loss: 1.72 \n",
            "Epoch 15, 10% \t train_loss: 1.72 \n",
            "Epoch 15, 20% \t train_loss: 1.71 \n",
            "Epoch 15, 30% \t train_loss: 1.71 \n",
            "Epoch 15, 40% \t train_loss: 1.71 \n",
            "Epoch 15, 51% \t train_loss: 1.71 \n",
            "Epoch 15, 61% \t train_loss: 1.71 \n",
            "Epoch 15, 71% \t train_loss: 1.72 \n",
            "Epoch 15, 81% \t train_loss: 1.71 \n",
            "Epoch 15, 92% \t train_loss: 1.72 \n",
            "Epoch 16, 10% \t train_loss: 1.71 \n",
            "Epoch 16, 20% \t train_loss: 1.70 \n",
            "Epoch 16, 30% \t train_loss: 1.70 \n",
            "Epoch 16, 40% \t train_loss: 1.71 \n",
            "Epoch 16, 51% \t train_loss: 1.70 \n",
            "Epoch 16, 61% \t train_loss: 1.71 \n",
            "Epoch 16, 71% \t train_loss: 1.71 \n",
            "Epoch 16, 81% \t train_loss: 1.71 \n",
            "Epoch 16, 92% \t train_loss: 1.71 \n",
            "Epoch 17, 10% \t train_loss: 1.70 \n",
            "Epoch 17, 20% \t train_loss: 1.70 \n",
            "Epoch 17, 30% \t train_loss: 1.69 \n",
            "Epoch 17, 40% \t train_loss: 1.70 \n",
            "Epoch 17, 51% \t train_loss: 1.71 \n",
            "Epoch 17, 61% \t train_loss: 1.70 \n",
            "Epoch 17, 71% \t train_loss: 1.71 \n",
            "Epoch 17, 81% \t train_loss: 1.71 \n",
            "Epoch 17, 92% \t train_loss: 1.72 \n",
            "Epoch 18, 10% \t train_loss: 1.70 \n",
            "Epoch 18, 20% \t train_loss: 1.71 \n",
            "Epoch 18, 30% \t train_loss: 1.69 \n",
            "Epoch 18, 40% \t train_loss: 1.70 \n",
            "Epoch 18, 51% \t train_loss: 1.70 \n",
            "Epoch 18, 61% \t train_loss: 1.70 \n",
            "Epoch 18, 71% \t train_loss: 1.70 \n",
            "Epoch 18, 81% \t train_loss: 1.69 \n",
            "Epoch 18, 92% \t train_loss: 1.70 \n",
            "Epoch 19, 10% \t train_loss: 1.69 \n",
            "Epoch 19, 20% \t train_loss: 1.68 \n",
            "Epoch 19, 30% \t train_loss: 1.68 \n",
            "Epoch 19, 40% \t train_loss: 1.69 \n",
            "Epoch 19, 51% \t train_loss: 1.69 \n",
            "Epoch 19, 61% \t train_loss: 1.70 \n",
            "Epoch 19, 71% \t train_loss: 1.70 \n",
            "Epoch 19, 81% \t train_loss: 1.69 \n",
            "Epoch 19, 92% \t train_loss: 1.70 \n",
            "Epoch 20, 10% \t train_loss: 1.68 \n",
            "Epoch 20, 20% \t train_loss: 1.69 \n",
            "Epoch 20, 30% \t train_loss: 1.69 \n",
            "Epoch 20, 40% \t train_loss: 1.69 \n",
            "Epoch 20, 51% \t train_loss: 1.69 \n",
            "Epoch 20, 61% \t train_loss: 1.69 \n",
            "Epoch 20, 71% \t train_loss: 1.69 \n",
            "Epoch 20, 81% \t train_loss: 1.69 \n",
            "Epoch 20, 92% \t train_loss: 1.69 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FbuBijb4MaoX",
        "colab_type": "code",
        "outputId": "abcde654-3121-4985-a970-292d99828f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on test images: 65 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4e13P1OT65sB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_state_dict('mytraining.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8hy6xwh6wP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**UNDERSTANDING RESIDUAL NETWORKS**\n",
        "\n",
        "The paper on deep residual learning for image recognition gives clear evidence about the degradation problem associated with extremely deep non-residual networks. Basically such an architecture, which gives poor results on both test and training data, performs worse than a similar architecture but having less number of layers. Thus even though we increase the number of layers, the performance actually degrades.\n",
        "\n",
        "The paper also provides evidence to support the fact that this problem is not caused by overfitting since the training errror also increases on increasing the number of layers.\n",
        "\n",
        "An explanation for this can be given by the fact that if a certain number of layers in a deep network are able to learn the required function quite accurately, then increasing the number of layers in the network would give best results only if the rest of the layers learn the identity function. But, according to the data given in the paper, it can be clearly seen that learning identity functions is not very easy for a very deep network. Even increasing the number of iterations significantly does not have much effect on the training and test set accuracy.\n",
        "\n",
        "On the other hand implementing a residual deep network solves nearly all the problems. In this architecture the extra added layers have to learn the zero mapping instead of the identity mapping which can be learnt much easily. In a residual network a shortcut connection is added which skips one layer to make the above mentioned change. Thus, if we add a residual layer in between layer l  and layer l+2, the input of layer l+2 will consist of W2*sigma(W1*x) + x.\n",
        "The extra 'x' term does not introduce any extra parameter and hence does not increase the complexity of the network.\n",
        "\n",
        "In case the input and output dimensions are not same 'x' may be padded with extra zeros or multiplied by a projection matrix.\n",
        "\n",
        "The projection shorcuts introduces a new parameter, but model does only marginally better due to this new parameter intoduced.\n",
        "\n",
        "After comparing the models developed on various datasets using residual and non-residual architectures, it becomes clear that residual networks do not face the problem of degradation encountered by plain networks.\n",
        "\n",
        "The bootleneck design which replaces two 3X3 layers with 1 1X1, 1 3X3 and another 1X1 can be used as an efficient tool while training to reduce the training time.\n",
        "\n",
        "The residual networks work well for object detection tasks as well which is clearly indicated by its performance on various datasets."
      ]
    },
    {
      "metadata": {
        "id": "wRZ_kXZ2_v8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}